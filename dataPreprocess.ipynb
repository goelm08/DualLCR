{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T20:30:11.430683164Z",
     "start_time": "2023-06-04T20:28:59.147945886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from datasets.preprocess import mask_citations, tokenize_and_index\n",
    "with open('../data/graph_lda_arxiv.gpickle' ,'rb') as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# papers.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T18:13:26.903389686Z",
     "start_time": "2023-06-04T18:12:45.538953723Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to calculate in-degree for a single node\n",
    "def calculate_in_degree(node):\n",
    "    return node, G.in_degree(node)\n",
    "\n",
    "# Calculate in-degree for all nodes using parallel processing\n",
    "def calculate_in_degrees_parallel(graph):\n",
    "    with Pool(processes=100) as pool:\n",
    "        results = pool.map(calculate_in_degree, graph.nodes())\n",
    "\n",
    "    return dict(results)\n",
    "\n",
    "# Call the function to calculate in-degree for all nodes\n",
    "in_degrees = calculate_in_degrees_parallel(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T18:13:26.904027121Z",
     "start_time": "2023-06-04T18:13:26.903161201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.nodes['2009.06394']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T19:21:33.164312615Z",
     "start_time": "2023-06-04T19:21:29.058581811Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504661\n"
     ]
    }
   ],
   "source": [
    "authors_all = { ' '.join(reversed(author_info)) for node in G.nodes for author_info in G.nodes[node].get('authors_parsed', []) }\n",
    "print(len(authors_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T19:23:36.797422060Z",
     "start_time": "2023-06-04T19:23:03.417989205Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert authors_all to a dictionary\n",
    "authors_all_dict = {name: idx for idx, name in enumerate(authors_all)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.preprocess import tokenize_and_index\n",
    "word2index = {}\n",
    "c = 0\n",
    "with open('resources/ai2_embeddings.txt') as f:\n",
    "    next(f)  # skip the first line\n",
    "    for line in f:\n",
    "        word2index[line.split()[0]] = c\n",
    "        c += 1\n",
    "vocabulary = set(list(word2index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479645/479645 [02:28<00:00, 3240.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to process a node\n",
    "def process_node(node):\n",
    "    json_data = {}\n",
    "    node_data = G.nodes[node]\n",
    "    try:\n",
    "        if node_data and node_data.get('abstract') and node_data.get('title'):\n",
    "            authors = [' '.join(reversed(author_info)) for author_info in node_data.get('authors_parsed', [])]\n",
    "            authors_ids = [authors_all_dict.get(author_name) for author_name in authors if authors_all_dict.get(author_name) is not None]\n",
    "\n",
    "            json_data[node] = {\n",
    "                'preprocessed_abstract': tokenize_and_index(node_data.get('abstract'), word2index, vocabulary, split_sentences=True),\n",
    "                'preprocessed_title': tokenize_and_index(node_data.get('title'), word2index, vocabulary),\n",
    "                'title': node_data.get('title'),\n",
    "                'abstract': node_data.get('abstract'),\n",
    "                'authors': authors,\n",
    "                'citations_per_year': {},\n",
    "                'authors_ids': authors_ids,\n",
    "                'total_citations': in_degrees.get(node, 0)\n",
    "            }\n",
    "        return json_data\n",
    "    except Exception as e:\n",
    "        print(node.get('title'), e)\n",
    "\n",
    "# Create a pool of processes\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "\n",
    "# Map the function to the nodes using multiple processes with a progress bar\n",
    "results = []\n",
    "with tqdm(total=len(G.nodes)) as pbar:\n",
    "    for result in pool.imap_unordered(process_node, G.nodes):\n",
    "        results.append(result)\n",
    "        pbar.update()\n",
    "\n",
    "# Close the pool to prevent any more tasks from being submitted\n",
    "pool.close()\n",
    "\n",
    "# Wait for all the processes to finish\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the papers.json file\n",
    "with open('arxiv/papers.json') as f:\n",
    "    papers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ids_dataset = set(papers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = {}\n",
    "paper_ids_data = {}\n",
    "for result in results:\n",
    "    try:\n",
    "        if result is not None:\n",
    "            json_data[list(result.keys())[0]] = list(result.values())[0]\n",
    "            paper_ids_data[list(result.keys())[0]] = True\n",
    "    except Exception as e:\n",
    "        print(result,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T19:24:18.187328655Z",
     "start_time": "2023-06-04T19:24:05.476488565Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to arxiv/papers.json\n"
     ]
    }
   ],
   "source": [
    "# Define the path of the JSON file\n",
    "json_file_path = 'arxiv/papers.json'\n",
    "\n",
    "# Save the JSON data to the file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "\n",
    "print(\"JSON data saved to\", json_file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Contexts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T19:48:55.971768050Z",
     "start_time": "2023-06-04T19:48:55.961507899Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': [{'heading': 'Altruism',\n",
       "   'text': 'Extensive previous and ongoing work has been dedicated to estimating reward functions and interactive parameters (Albrecht, Crandall, and Ramamoorthy 2016;Albrecht and Stone 2019;Albrecht et al. 2020;Schwarting et al. 2019). In this work we presume that the \"true\" reward matrix {(r mn1 , r mn2 )} 0<m≤M,0<n≤N , and altruism values α 1 , α 2 are known to both agents. Each agent can then, independently, construct the reward matrix {(r * mn1 , r * mn2 )} 0<m≤M,0<n≤N , which they will use to choose which intention to follow.'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G['2009.06394']['1507.07688']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(G.edges(data=True))\n",
    "sample_edges = edges[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets.preprocess import mask_citations\n",
    "\n",
    "def process_edge(edge):\n",
    "    try:\n",
    "        source, target, data = edge\n",
    "        result = {}\n",
    "        for i, item in enumerate(data['meta']):\n",
    "            if source in papers_ids_dataset and target in papers_ids_dataset and item['text'] is not None:\n",
    "                key = f\"{source}_{target}_{i}\"\n",
    "                masked_text = mask_citations(item['text'], dataset='acl')\n",
    "                preprocessed_text = tokenize_and_index(masked_text, word2index, vocabulary)\n",
    "                result[key] = {\n",
    "                    \"preprocessed\": preprocessed_text,\n",
    "                    \"masked_text\": masked_text,\n",
    "                    \"tc_index\": preprocessed_text.index(word2index['targetcit']),\n",
    "                    \"citation_context\": item['text'],\n",
    "                    \"ref_id\": target,\n",
    "                    \"citing_id\": source\n",
    "                }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4452945/4452945 [43:10<00:00, 1719.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a pool of processes\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count() - 20)\n",
    "# pool = multiprocessing.Pool()\n",
    "\n",
    "# Create a list of edges to process\n",
    "\n",
    "# Process the edges using multiple processes with a progress bar\n",
    "results = []\n",
    "with tqdm(total=len(edges)) as pbar:\n",
    "    for result in pool.imap_unordered(process_edge, edges):\n",
    "        results.append(result)\n",
    "        pbar.update()\n",
    "\n",
    "# Close the pool to prevent any more tasks from being submitted\n",
    "pool.close()\n",
    "\n",
    "# Wait for all the processes to finish\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = {}\n",
    "for result in results:\n",
    "    try:\n",
    "        for key, value in result.items():\n",
    "            json_data[key] = value\n",
    "    except Exception as e:\n",
    "        print(result,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_dataset = set(json_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T20:36:40.325704757Z",
     "start_time": "2023-06-04T20:35:02.849372035Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the path of the JSON file\n",
    "json_file_path = 'arxiv/contexts.json'\n",
    "\n",
    "# Save the JSON data to the file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "\n",
    "print(\"JSON data saved to\", json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arxiv/contexts.json') as f:\n",
    "    contexts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'preprocessed_text' with 'preprocessed' in the contexts.json file\n",
    "for key, value in contexts.items():\n",
    "    contexts[key]['preprocessed'] = contexts[key]['preprocessed_text']\n",
    "    del contexts[key]['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('arxiv/contexts_new.json', 'w') as json_file:\n",
    "    json.dump(contexts, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train val test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "citations_dataset_list = list(contexts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2103.04556_1609.00451_0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'quant-ph/0611021' in citations_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_key(key):\n",
    "    source, target, index = key.split('_')\n",
    "    return {'context_id': key, 'paper_id': target, 'date': G.nodes[target]['date']}\n",
    "\n",
    "# Create a pool of processes\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Map the function to the keys using multiple processes\n",
    "results = pool.map(process_key, citations_dataset_list)\n",
    "\n",
    "# Close the pool to prevent any more tasks from being submitted\n",
    "pool.close()\n",
    "\n",
    "# Wait for all the processes to finish\n",
    "pool.join()\n",
    "\n",
    "# Combine the results from all processes\n",
    "result = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400564\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = [{'context_id': f'{source}_{target}_{i}', 'paper_id': target, 'date': G.nodes[target]['date']}\n",
    "          for source, target, data in G.edges(data=True)\n",
    "          for i, _ in enumerate(data['meta'])]\n",
    "\n",
    "# Print JSON\n",
    "# json_citation = json.dumps(result, indent=4)\n",
    "print(len(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': [{'heading': 'Altruism',\n",
       "   'text': 'Extensive previous and ongoing work has been dedicated to estimating reward functions and interactive parameters (Albrecht, Crandall, and Ramamoorthy 2016;Albrecht and Stone 2019;Albrecht et al. 2020;Schwarting et al. 2019). In this work we presume that the \"true\" reward matrix {(r mn1 , r mn2 )} 0<m≤M,0<n≤N , and altruism values α 1 , α 2 are known to both agents. Each agent can then, independently, construct the reward matrix {(r * mn1 , r * mn2 )} 0<m≤M,0<n≤N , which they will use to choose which intention to follow.'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G['2009.06394']['1507.07688']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "result = [item for item in result if item['paper_id'][0].isdigit()]\n",
    "result = sorted(result, key=lambda x: datetime.strptime(x['date'], '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 8303616\n",
      "Train size: 5812531\n",
      "Validation size: 1245542\n",
      "Test size: 1245543\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes for each split\n",
    "total_size = len(result)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "\n",
    "# Split the data\n",
    "train_data = result[:train_size]\n",
    "val_data = result[train_size:train_size+val_size]\n",
    "test_data = result[train_size+val_size:]\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Total size: {len(result)}\")\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_Val_data = []\n",
    "for obj in val_data:\n",
    "    obj['true_ref'] = obj.pop('paper_id')\n",
    "    obj['neg_ref'] = '2206.06468'\n",
    "    modified_Val_data.append(obj)\n",
    "# modified_Val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start date 2007-05-23\n",
      "train end date 2019-12-05\n",
      "val start date 2019-12-05\n",
      "val end date 2021-01-11\n",
      "test start date 2021-01-11\n",
      "test end date 2023-01-27\n"
     ]
    }
   ],
   "source": [
    "print('train start date', train_data[0]['date'])\n",
    "print('train end date', train_data[-1]['date'])\n",
    "print('val start date', modified_Val_data[0]['date'])\n",
    "print('val end date', modified_Val_data[-1]['date'])\n",
    "print('test start date', test_data[0]['date'])\n",
    "print('test end date', test_data[-1]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train data to a file\n",
    "with open('arxiv/train_data.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Save validation data to a file\n",
    "with open('arxiv/val_data.json', 'w') as val_file:\n",
    "    json.dump(modified_Val_data, val_file, indent=4)\n",
    "\n",
    "# Save test data to a file\n",
    "with open('arxiv/test_data.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.06468\n"
     ]
    }
   ],
   "source": [
    "## for val.json neg_ref\n",
    "\n",
    "for node in G.nodes():\n",
    "    if G.in_degree(node) == 0 and G.out_degree(node) == 0:\n",
    "        print(node)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
